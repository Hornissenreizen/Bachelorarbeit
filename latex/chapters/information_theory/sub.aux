\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Information Theory}{1}{section.1}\protected@file@percent }
\newlabel{section:information_theory}{{1}{1}{Information Theory}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Entropy}{1}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plot of the function $y=-x\qopname  \relax o{ln}(x)$.}}{1}{figure.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Joint, Conditional, and Cross Entropy}{3}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Properties of Entropy}{4}{subsubsection.1.1.2}\protected@file@percent }
\newlabel{proposition:entropy_conditional}{{1.3}{4}{}{proposition.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Kullback-Leibler Divergence}{6}{subsection.1.2}\protected@file@percent }
\newlabel{sec:kullback_leibler_divergence}{{1.2}{6}{Kullback-Leibler Divergence}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Mutual Information}{8}{subsection.1.3}\protected@file@percent }
\newlabel{sec:mutual_information}{{1.3}{8}{Mutual Information}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Data Processing Inequality}{11}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{theorem:data_processing_inequality}{{1.3}{11}{Data Processing Inequality}{theorem.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Bounding Mutual Information via Matrix Rank of the Joint Distribution}{12}{subsection.1.4}\protected@file@percent }
\newlabel{theorem:mutual_information_is_bounded_by_log_rank}{{1.4}{12}{}{theorem.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Convergence of Mutual Information}{13}{subsection.1.5}\protected@file@percent }
\newlabel{theorem:element-wise_exponential_convergence_implies_exponential_convergence}{{1.5}{13}{Element-Wise Exponential Convergence Implies Exponential Convergence}{theorem.1.5}{}}
\newlabel{lemma:xlogx_exponential_convergence}{{1.4}{15}{}{lemma.1.4}{}}
\newlabel{corollary:exponential_convergence_of_conditional_probability_matrix_implies_exponential_convergence_of_mutual_information}{{1.9}{16}{}{corollary.1.9}{}}
\gdef \@abspage@last{16}
