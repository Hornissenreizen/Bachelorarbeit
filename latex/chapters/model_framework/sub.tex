\documentclass[../../main.tex]{subfiles}

\begin{document}
\section{Model Framework}
    We are interested in models with asymptotically power-law decay of the mutual information measure with respect to the distance between the tokens in the sequence. So far so good. But what does it \emph{actually} mean?

    The tokens, represented by random variables $X_t$, are elements of a finite alphabet $\Sigma$. The distance between $X_t$ and $X_{t + \tau}$ is $\tau$, and for every $t$ and every $\tau$ we want to bound
    \[
        I(X_t, X_{t + \tau}) \in \Omega(\tau^{-\alpha}), \ I(X_t, X_{t + \tau}) \in \mathcal{O}(\tau^{-\beta}) \quad ,
    \]
    for some fixed $\alpha, \beta \in \mathbb{R}_{>0}$. The first condition is the important one, while the latter ensures that $I(X_t, X_{t + \tau}) \xrightarrow{\tau \to \infty} 0$. We also may replace the latter condition by this one.

    This was straight forward. The challenging part is to define what a  model is. In the case of Markov chains this seems trivial: We define a finite set of parameters (the transition probabilities), and we get a model over $\Sigma^*$, that is for every $n \in \mathbb{N}$ the model defines a probability measure over $\Sigma^n$. Thus, the first conclusion is every model $S$ must define a probability measure over $\Sigma^n$ for every $n\in \mathbb{N}$.

    As a first formalization, $S$ is a function $S: (n, w) \mapsto [0, 1]$, for $n \in \mathbb{N}, \ w \in \Sigma^n$ s.t. $\sum_{w \in \Sigma^n} S(n, w) = 1$.

    But really, we want to restrain $S$ in order to have reasonable time and space complexity, and to ensure the model is \emph{reasonable}, which means that the language of $S_n(w)$ should look \emph{similar} to $S_{n + d}(w)$, whatever this might mean, where we used the notation $S_n(w) \equiv S(n, w)$. We also write $w_i$ for $X_i$. Really, $w$ is a 1-indexed String of $X_i$.  

    We present one strict definition for this \emph{similarity} in the following definition:

    \begin{definition}
        We say $S$ is \emph{well behaved} iff for every $n \in \mathbb{N}, \ w \in \Sigma^{n + 1}$ it holds true that
        \[
            \sum_{w_{n + 1} \in \Sigma} S_{n + 1}(w) = S_n(w_{-(n + 1)}) \quad .
        \]
    \end{definition}

    \begin{remark}
        Markov chains and hidden Markov models are well behaved.
    \end{remark}

    \begin{lemma}
        For every $d \in \mathbb{N}$, let $I \coloneqq [n + d] \setminus [n] = \{ n+1, \dots, n + d \}$. Then, if $S$ is well behaved, we have for every $w \in \Sigma^{n + d}$:
        \[
            \sum_{w_I \in \Sigma^d} S_{n + d}(w) = S_n(w_{-I}) \quad .
        \]
    \end{lemma}
    % \vspace{-2.5em}
    \begin{proof}
        We use induction over $d$. The base case follows directly from the definition of $S$ being well behaved. Thus, assume the claim holds for some $d \coloneqq k$. Then we have
        \begin{align*}
            \sum_{w_I \in \Sigma^{k + 1}} S_{n + k + 1}(w) &= \sum_{w_{I \setminus \{ k + 1 \}} \in \Sigma^{k}} \sum_{w_{k + 1} \in \Sigma} S_{n + k + 1}(w) \\
            &\underset{S \text{ well defined}}{=} \sum_{w_{I \setminus \{ k + 1 \}} \in \Sigma^{k}} S_{n + k}(w_{-\{ k + 1 \}}) \\
            &\underset{\text{induction hypothesis}}{=} S_n(w_{-I}) \quad ,
        \end{align*}
        which concludes the induction step.
    \end{proof}

    \begin{definition}
        From any model $S$, we can construct the \emph{induced well behaved} model $S^*$ by defining $S_n^*$ recursively as 
        \begin{itemize}
            \item $S_1^* \coloneqq S_1$ ,
            \item $S_{n + 1}^*(w) \coloneqq S_n^*(w_{-\{n+1\}}) \dfrac{S_{n+1}(w)}{\sum_{w_{n+1} \in \Sigma} S_{n+1}(w)}$ .
        \end{itemize}
    \end{definition}

    \begin{lemma}
        The induced well behaved model $S^*$ is indeed well behaved.
    \end{lemma}
    \vspace{-2.5em}
    \begin{proof}
        We have:
        \begin{align*}
            \sum_{w_{n + 1} \in \Sigma} S_{n + 1}^*(w) &= \sum_{w_{n + 1} \in \Sigma} S_n^*(w_{-\{n+1\}}) \dfrac{S_{n+1}(w)}{\sum_{w_{n+1} \in \Sigma} S_{n+1}(w)} \\
            &= \dfrac{S_n^*(w_{-\{n+1\}})}{\sum_{w_{n+1} \in \Sigma} S_{n+1}(w)} \sum_{w_{n + 1} \in \Sigma} S_{n+1}(w) \\
            &\overset{\checkmark}{=} S_n^*(w_{-\{n+1\}}) \quad .
        \end{align*}
    \end{proof}

    Now, we want to look at how we might restrict our model $(S_n)_{n \in \mathbb{N}} \equiv S$. One approach might be to define a model structure for every $n \in \mathbb{N}$ with finite parameters $\bm{\theta}_n$, thus $S_n \in \{ S_n(\bm{\theta}_n) : \bm{\theta}_n \in \Theta_n \} \eqqcolon \mathcal{S}_n$. We write $S_{n, \bm{\theta}_n}$ for $S_n$ with parameters $\bm{\theta}_n$. Hence, $(S_n)_{n \in \mathbb{N}}$ is completely defined by $(\mathcal{S}_n, \bm{\theta}_n)_{n \in \mathbb{N}}$. We call $\mathcal{S}(n) \equiv \mathcal{S}_n$ the \emph{model space}.

    \begin{definition}
        We say $(S_n)_{n \in \mathbb{N}}$ is a family of models over the model space $\mathcal{S}$ iff $S_n \in \mathcal{S}_n$ for every $n \in \mathbb{N}$. As a shorthand, we write $S \in \mathcal{S}$.
    \end{definition}

    For our model $S$, we want the mutual information between \emph{any} two variables $X_t$, $X_{t + \tau}$ to be $I(X_t, X_{t + \tau}) \in \Omega(\tau^{-\alpha})$, i.e. it has to hold for every $t$ and \emph{every} $S_n$. Thus, let us define the following:

    \begin{definition}
        We define $i_{S_n}(\tau)$ and $I_{S_n}(\tau)$ to be the minimal and maximal mutual information between any two variables of $S_n$ with distance $\tau$. Formally, let $X_t, X_{t + \tau}$ ($t + \tau \leq n$) be random variables with distributions defined by $S_n$. Then:
        \vspace{-1em}
        \begin{itemize}
            \item $i_{S_n}(\tau) \coloneqq \min_{t \in [n - \tau]} I(X_t, X_{t + \tau}) \quad ,$
            \item $I_{S_n}(\tau) \coloneqq \max_{t \in [n - \tau]} I(X_t, X_{t + \tau}) \quad .$
        \end{itemize}
    \end{definition}

    \begin{definition}
        A model $S$ has \emph{lower bound power-law behavior} iff there exist constants $c, \alpha \in \mathbb{R}_{>0}$ s.t. for every $n \in \mathbb{N}$ it holds true that $i_{S_n}(\tau) \geq c \tau^{-\alpha}$. Similarly, $S$ has \emph{upper bound power-law behavior} iff there exist constants $c', \alpha' \in \mathbb{R}_{>0}$ s.t. for every $n \in \mathbb{N}$ it holds true that $I_{S_n}(\tau) \leq c' \tau^{-\alpha'}$. Furthermore, $S$ has \emph{decaying behavior} iff for every $n \in \mathbb{N}$ we have $I_{S_{n + \tau}}(\tau) \xrightarrow{\tau \to \infty} 0$. Lastly, $S$ has \emph{power-law behavior} iff it has lower and upper bound power-law behavior (alternatively decaying behavior instead of upper bound power-law behavior).
    \end{definition}

    \begin{definition}
        We define $\overline{i_{S_n}}$ and $\overline{I_{S_n}}$ to be the minimal and maximal mutual information between any two variables of $S_n$ with arbitrary distance $\tau$. Formally, let $X_i, X_j$ ($1 \leq i < j \leq n$) be random variables with distributions defined by $S_n$. Then:
        \vspace{-1em}
        \begin{itemize}
            \item $\overline{i_{S_n}} \coloneqq \min_{(i, j) \in [n]^2, i < j} I(X_i, X_j) = \min_{\tau \in [n - 1]} i_{S_n}(\tau) \quad ,$
            \item $\overline{I_{S_n}} \coloneqq \max_{(i, j) \in [n]^2, i < j} I(X_i, X_j) = \max_{\tau \in [n - 1]} I_{S_n}(\tau) \quad .$
        \end{itemize}
    \end{definition}

    \begin{theorem}
        Let $S^*$ be a well behaved model. Then $S^*$ has lower bound power-law behavior iff $\overline{i_{S_n}} \equiv \overline{i_S}(n) \in \Omega(n^{-\alpha})$ for some $\alpha \in \mathbb{R}_{>0}$.
    \end{theorem}
\end{document}