\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Model Framework}{3}{section.1}\protected@file@percent }
\newlabel{lemma:random_variables_do_not_change_with_future_models}{{1.1}{4}{}{lemma.1.1}{}}
\newlabel{def:induced_bulk_marginal_model}{{1.3}{4}{Induced Bulk Marginal Model}{definition.1.3}{}}
\newlabel{definition:strong_model_power_law_behavior}{{1.6}{5}{Strong Power-Law Behavior}{definition.1.6}{}}
\newlabel{definition:weak_power_law_behavior}{{1.8}{6}{Weak Power-Law Behavior}{definition.1.8}{}}
\newlabel{theorem:power_law_decay_in_well-behaved_models_with_weak_power-law_behavior}{{1.1}{6}{Every Token has Power-Law Decay in Models with the Bulk Marginal Property and Weak Power-Law Behavior}{theorem.1.1}{}}
\newlabel{proposition:strong_slbplb_implies_wlbplb}{{1.2}{7}{}{proposition.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}A Model with Power-Law Behavior}{8}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Mutual Information in Markov Chains}{9}{section.3}\protected@file@percent }
\newlabel{sec:mutual_information_in_markov_chains}{{3}{9}{Mutual Information in Markov Chains}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Exponential Decay in Irreducible Aperiodic Markov Chains}{9}{subsection.3.1}\protected@file@percent }
\newlabel{theorem:no_power-law_in_irreducible_aperdioc_markov_chains}{{3.1}{9}{No Power-Law in irreducible aperiodic Markov Chains}{theorem.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simple irreducible aperiodic Markov chain. Note that if $X_{t_0} = C$, then we know that $X_{t_0+1} = A$.}}{10}{figure.1}\protected@file@percent }
\newlabel{fig:2_markov_chain}{{1}{10}{A simple irreducible aperiodic Markov chain. Note that if $X_{t_0} = C$, then we know that $X_{t_0+1} = A$}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}The Defective Case}{14}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{section:the_defectvie_case}{{3.1.1}{14}{The Defective Case}{subsubsection.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}No Markov Chain with Power-Law Behavior}{15}{subsection.3.2}\protected@file@percent }
\newlabel{theorem:no_markov_chain_with_power-law_behavior}{{3.2}{16}{No Markov Chain with Power-Law Behavior}{theorem.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}No Power-Law in Hidden Markov Models}{18}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bayesian network of Markov chains. All arrows go from previous tokens to future tokens.}}{18}{figure.2}\protected@file@percent }
\newlabel{fig:bayesian_network_markov_chain}{{2}{18}{Bayesian network of Markov chains. All arrows go from previous tokens to future tokens}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Bayesian network of a hidden Markov model.}}{19}{figure.3}\protected@file@percent }
\newlabel{fig:bayesian_network_hidden_markov}{{3}{19}{Bayesian network of a hidden Markov model}{figure.3}{}}
\newlabel{lemma:raising_marov_matrix_to_its_period}{{4.1}{19}{}{lemma.4.1}{}}
\newlabel{lemma:irreducible_aperiodic_markov_chain_stays_irreducible_aperiodic}{{4.2}{20}{}{lemma.4.2}{}}
\newlabel{lemma:exponential_convergence_with_open_states}{{4.4}{21}{}{lemma.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adjusted Bayesian network of a hidden Markov model.}}{23}{figure.4}\protected@file@percent }
\newlabel{fig:adjusted_bayesian_network_hidden_markov}{{4}{23}{Adjusted Bayesian network of a hidden Markov model}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Conclusions for Model Selection}{25}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Binary Tree Tensor Networks}{27}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Binary tree model space for sequences of length $n = 2^k$.}}{27}{figure.5}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network}{{5}{27}{Binary tree model space for sequences of length $n = 2^k$}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Bulk Marginal Property}{27}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Bulk marginal property enforces the equivalence of these models.}}{28}{figure.6}\protected@file@percent }
\newlabel{fig:bmp_model_equiv}{{6}{28}{Bulk marginal property enforces the equivalence of these models}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Contracting this sub-network with all-ones vectors yields vector $\bm  {v}$.}}{28}{figure.7}\protected@file@percent }
\newlabel{fig:sufficient_condition_bmp}{{7}{28}{Contracting this sub-network with all-ones vectors yields vector $\bm {v}$}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Binary Tree Tensor Networks are Universal Approximators}{29}{subsection.5.2}\protected@file@percent }
\newlabel{theorem:bttn_are_universal_approximators}{{5.1}{29}{}{theorem.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Model structure of binary tree tensor networks for $n = 2^k = 4$.}}{30}{figure.8}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network_n_equals_four}{{8}{30}{Model structure of binary tree tensor networks for $n = 2^k = 4$}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Restricting Parameters}{30}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}PCBTTN Aren't Universal Approximators}{31}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Cut in a binary tree tensor network.}}{32}{figure.9}\protected@file@percent }
\newlabel{pic:cut_bttn}{{9}{32}{Cut in a binary tree tensor network}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Power-Law Behavior in PCBTTN}{32}{subsection.5.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Model structure of binary tree tensor networks for $n = 2^k = 4$.}}{33}{figure.10}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network_n_equals_four_2}{{10}{33}{Model structure of binary tree tensor networks for $n = 2^k = 4$}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Markov Chains}{35}{appendix.A}\protected@file@percent }
\newlabel{section:markov_chains}{{A}{35}{Markov Chains}{appendix.A}{}}
\newlabel{ex:markov_chain}{{A.1}{35}{Markov Transition Matrix}{example.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Properties}{36}{subsection.A.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Graph representation of the Markov chain defined in example\nobreakspace  {}\ref  {ex:markov_chain}.}}{37}{figure.11}\protected@file@percent }
\newlabel{fig:markov_chain}{{11}{37}{Graph representation of the Markov chain defined in example~\ref {ex:markov_chain}}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.1}Irreducibility}{37}{subsubsection.A.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Graph of a reducible Markov chain. Note that once the chain transitions from state $1$ to state $3$, it will stay at state $3$ indefinitely.}}{37}{figure.12}\protected@file@percent }
\newlabel{fig:markov_chain_reducible}{{12}{37}{Graph of a reducible Markov chain. Note that once the chain transitions from state $1$ to state $3$, it will stay at state $3$ indefinitely}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.2}Aperiodicity}{39}{subsubsection.A.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Graph of a periodic Markov chain. Note that once once the starting position is determined, then we also know the state after $t$ steps.}}{39}{figure.13}\protected@file@percent }
\newlabel{fig:markov_chain_periodic}{{13}{39}{Graph of a periodic Markov chain. Note that once once the starting position is determined, then we also know the state after $t$ steps}{figure.13}{}}
\newlabel{lemma:aux}{{A.3}{40}{}{lemma.A.3}{}}
\newlabel{remark:converse_lemma_aux}{{A.5}{40}{}{remark.A.5}{}}
\newlabel{lemma:return_states_with_period}{{A.4}{40}{}{lemma.A.4}{}}
\newlabel{proposition:period_is_a_class_property}{{A.1}{40}{Period is a Class Property}{proposition.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Irreducible Aperiodic Markov Chains}{41}{subsection.A.2}\protected@file@percent }
\newlabel{theorem:positive_transition_matrix}{{A.3}{41}{Positive n-Step Transition Matrix for Irreducible Aperiodic Markov Chains}{theorem.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Graph of a reducible Markov chain with two closed communication classes. Note that we might end up stuck at either state $2$ or state $3$.}}{42}{figure.14}\protected@file@percent }
\newlabel{fig:markov_chain_two_closed_classes}{{14}{42}{Graph of a reducible Markov chain with two closed communication classes. Note that we might end up stuck at either state $2$ or state $3$}{figure.14}{}}
\newlabel{corollary:converse_positive_transition_matrix}{{A.2}{42}{}{corollary.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Perron-Frobenius Theorem}{42}{subsection.A.3}\protected@file@percent }
\newlabel{theorem:perron_frobenius}{{A.4}{42}{Perron-Frobenius}{theorem.A.4}{}}
\newlabel{corollary:perron_frobenius_extension}{{A.3}{44}{Extension to More General Markov Chains}{corollary.A.3}{}}
\newlabel{lemma:perron_frobenius_extension_A_to_the_m}{{A.5}{45}{}{lemma.A.5}{}}
\newlabel{remark:exponential_decay}{{A.7}{45}{}{remark.A.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Information Theory}{47}{appendix.B}\protected@file@percent }
\newlabel{section:information_theory}{{B}{47}{Information Theory}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Entropy}{47}{subsection.B.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Plot of the function $y=-x\qopname  \relax o{ln}(x)$.}}{47}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.1}Joint, Conditional, and Cross Entropy}{49}{subsubsection.B.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.2}Properties of Entropy}{50}{subsubsection.B.1.2}\protected@file@percent }
\newlabel{proposition:entropy_conditional}{{B.3}{50}{}{proposition.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Kullback-Leibler Divergence}{52}{subsection.B.2}\protected@file@percent }
\newlabel{sec:kullback_leibler_divergence}{{B.2}{52}{Kullback-Leibler Divergence}{subsection.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Mutual Information}{54}{subsection.B.3}\protected@file@percent }
\newlabel{sec:mutual_information}{{B.3}{54}{Mutual Information}{subsection.B.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3.1}Data Processing Inequality}{57}{subsubsection.B.3.1}\protected@file@percent }
\newlabel{theorem:data_processing_inequality}{{B.3}{57}{Data Processing Inequality}{theorem.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Bounding Mutual Information via Matrix Rank of the Joint Distribution}{58}{subsection.B.4}\protected@file@percent }
\newlabel{theorem:mutual_information_is_bounded_by_log_rank}{{B.4}{58}{}{theorem.B.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Convergence of Mutual Information}{59}{subsection.B.5}\protected@file@percent }
\newlabel{theorem:element-wise_exponential_convergence_implies_exponential_convergence}{{B.5}{59}{Element-Wise Exponential Convergence Implies Exponential Convergence}{theorem.B.5}{}}
\newlabel{lemma:xlogx_exponential_convergence}{{B.4}{61}{}{lemma.B.4}{}}
\newlabel{corollary:exponential_convergence_of_conditional_probability_matrix_implies_exponential_convergence_of_mutual_information}{{B.9}{62}{}{corollary.B.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Tensor Networks}{63}{appendix.C}\protected@file@percent }
\newlabel{section:tensor_networks}{{C}{63}{Tensor Networks}{appendix.C}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A basic tensor network over $\Sigma ^3$.}}{64}{figure.16}\protected@file@percent }
\newlabel{proposition:marginalized_tensor_network}{{C.1}{65}{}{proposition.C.1}{}}
\newlabel{corollary:normalized_tensor_network}{{C.1}{65}{}{corollary.C.1}{}}
\newlabel{proposition:contracting_over_shared_index}{{C.2}{66}{}{proposition.C.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Contracting multiple tensors over one shared index is equivalent to contracting them individually with a single copy tensor.}}{67}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Reshaping a Tensor (Matricization)}{68}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1.1}An Upper Bound for the Rank of Matrices in Tensor Networks}{68}{subsubsection.C.1.1}\protected@file@percent }
\newlabel{theorem:min_cut_caps_rank}{{C.1}{69}{}{theorem.C.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A generic tensor network with a cut partitioning the physical indices into $\mathcal  {I}_A$ (blue) and $\mathcal  {I}_B$ (red). The cut crosses bonds $e_1, e_2, e_3$.}}{69}{figure.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Contracting the subgraphs results in two tensors, $U$ and $V$, connected by the cut bonds.}}{69}{figure.19}\protected@file@percent }
\gdef \@abspage@last{70}
