\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Model Framework}{3}{section.1}\protected@file@percent }
\newlabel{lemma:random_variables_do_not_change_with_future_models}{{1.1}{4}{}{lemma.1.1}{}}
\newlabel{def:induced_bulk_marginal_model}{{1.3}{4}{Induced Bulk Marginal Model}{definition.1.3}{}}
\newlabel{definition:strong_model_power_law_behavior}{{1.6}{5}{Strong Power-Law Behavior}{definition.1.6}{}}
\newlabel{definition:weak_power_law_behavior}{{1.8}{6}{Weak Power-Law Behavior}{definition.1.8}{}}
\newlabel{theorem:power_law_decay_in_well-behaved_models_with_weak_power-law_behavior}{{1.1}{6}{Every Token has Power-Law Decay in Models with the Bulk Marginal Property and Weak Power-Law Behavior}{theorem.1.1}{}}
\newlabel{proposition:strong_slbplb_implies_wlbplb}{{1.2}{7}{}{proposition.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mutual Information in Markov Chains}{8}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Exponential Decay in Irreducible Aperiodic Markov Chains}{8}{subsection.2.1}\protected@file@percent }
\newlabel{theorem:no_power-law_in_irreducible_aperdioc_markov_chains}{{2.1}{8}{No Power-Law in irreducible aperiodic Markov Chains}{theorem.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simple irreducible aperiodic Markov chain. Note that if $X_{t_0} = C$, then we know that $X_{t_0+1} = A$.}}{9}{figure.1}\protected@file@percent }
\newlabel{fig:2_markov_chain}{{1}{9}{A simple irreducible aperiodic Markov chain. Note that if $X_{t_0} = C$, then we know that $X_{t_0+1} = A$}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}The Defective Case}{13}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{section:the_defectvie_case}{{2.1.1}{13}{The Defective Case}{subsubsection.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}No Markov Chain with Power-Law Behavior}{14}{subsection.2.2}\protected@file@percent }
\newlabel{theorem:no_markov_chain_with_power-law_behavior}{{2.2}{15}{No Markov Chain with Power-Law Behavior}{theorem.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}No Power-Law in Hidden Markov Models}{17}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bayesian network of Markov chains. All arrows go from previous tokens to future tokens.}}{17}{figure.2}\protected@file@percent }
\newlabel{fig:bayesian_network_markov_chain}{{2}{17}{Bayesian network of Markov chains. All arrows go from previous tokens to future tokens}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Bayesian network of a hidden Markov model.}}{18}{figure.3}\protected@file@percent }
\newlabel{fig:bayesian_network_hidden_markov}{{3}{18}{Bayesian network of a hidden Markov model}{figure.3}{}}
\newlabel{lemma:aperiodic_irreducible_stays_aperiodic}{{3.2}{19}{}{lemma.3.2}{}}
\newlabel{lemma:exponential_convergence_with_open_states}{{3.3}{19}{}{lemma.3.3}{}}
\newlabel{lemma:exponential_convergence_with_open_states}{{3.4}{19}{}{lemma.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adjusted Bayesian network of a hidden Markov model.}}{21}{figure.4}\protected@file@percent }
\newlabel{fig:adjusted_bayesian_network_hidden_markov}{{4}{21}{Adjusted Bayesian network of a hidden Markov model}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Conclusions for Model Selection}{24}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Binary Tree Tensor Networks}{25}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Binary tree model space for sequences of length $n = 2^k$.}}{25}{figure.5}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network}{{5}{25}{Binary tree model space for sequences of length $n = 2^k$}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Bulk Marginal Property}{25}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Bulk marginal property enforces the equivalence of these models.}}{26}{figure.6}\protected@file@percent }
\newlabel{fig:bmp_model_equiv}{{6}{26}{Bulk marginal property enforces the equivalence of these models}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Contracting this sub-network with all-ones vectors yields vector $\bm  {v}$.}}{26}{figure.7}\protected@file@percent }
\newlabel{fig:sufficient_condition_bmp}{{7}{26}{Contracting this sub-network with all-ones vectors yields vector $\bm {v}$}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Binary Tree Tensor Networks are Universal Approximators}{27}{subsection.4.2}\protected@file@percent }
\newlabel{theorem:bttn_are_universal_approximators}{{4.1}{27}{}{theorem.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Model structure of binary tree tensor networks for $n = 2^k = 4$.}}{28}{figure.8}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network_n_equals_four}{{8}{28}{Model structure of binary tree tensor networks for $n = 2^k = 4$}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Restricting Parameters}{28}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}PCBTTN Aren't Universal Approximators}{29}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Cut in a binary tree tensor network.}}{30}{figure.9}\protected@file@percent }
\newlabel{pic:cut_bttn}{{9}{30}{Cut in a binary tree tensor network}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Power-Law Behavior in PCBTTN}{30}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Model structure of binary tree tensor networks for $n = 2^k = 4$.}}{31}{figure.10}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network_n_equals_four_2}{{10}{31}{Model structure of binary tree tensor networks for $n = 2^k = 4$}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Markov Chains}{33}{appendix.A}\protected@file@percent }
\newlabel{section:markov_chains}{{A}{33}{Markov Chains}{appendix.A}{}}
\newlabel{ex:markov_chain}{{A.1}{33}{Markov Transition Matrix}{example.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Properties}{34}{subsection.A.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Graph representation of the Markov chain defined in example\nobreakspace  {}\ref  {ex:markov_chain}.}}{35}{figure.11}\protected@file@percent }
\newlabel{fig:markov_chain}{{11}{35}{Graph representation of the Markov chain defined in example~\ref {ex:markov_chain}}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.1}Irreducibility}{35}{subsubsection.A.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Graph of a reducible Markov chain. Note that once the chain transitions from state $1$ to state $3$, it will stay at state $3$ indefinitely.}}{35}{figure.12}\protected@file@percent }
\newlabel{fig:markov_chain_reducible}{{12}{35}{Graph of a reducible Markov chain. Note that once the chain transitions from state $1$ to state $3$, it will stay at state $3$ indefinitely}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.2}Aperiodicity}{37}{subsubsection.A.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Graph of a periodic Markov chain. Note that once once the starting position is determined, then we also know the state after $t$ steps.}}{37}{figure.13}\protected@file@percent }
\newlabel{fig:markov_chain_periodic}{{13}{37}{Graph of a periodic Markov chain. Note that once once the starting position is determined, then we also know the state after $t$ steps}{figure.13}{}}
\newlabel{lemma:aux}{{A.3}{38}{}{lemma.A.3}{}}
\newlabel{remark:converse_lemma_aux}{{A.5}{38}{}{remark.A.5}{}}
\newlabel{lemma:period_is_a_class_property}{{A.4}{38}{Period is a Class Property}{lemma.A.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Irreducible Aperiodic Markov Chains}{39}{subsection.A.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Graph of a reducible Markov chain with two closed communication classes. Note that we might end up stuck at either state $2$ or state $3$.}}{39}{figure.14}\protected@file@percent }
\newlabel{fig:markov_chain_two_closed_classes}{{14}{39}{Graph of a reducible Markov chain with two closed communication classes. Note that we might end up stuck at either state $2$ or state $3$}{figure.14}{}}
\newlabel{theorem:positive_transition_matrix}{{A.3}{39}{Positive n-Step Transition Matrix for Irreducible Aperiodic Markov Chains}{theorem.A.3}{}}
\newlabel{corollary:converse_positive_transition_matrix}{{A.2}{40}{}{corollary.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Perron-Frobenius Theorem}{40}{subsection.A.3}\protected@file@percent }
\newlabel{theorem:perron_frobenius}{{A.4}{40}{Perron-Frobenius}{theorem.A.4}{}}
\newlabel{corollary:perron_frobenius_extension}{{A.3}{42}{Extension to More General Markov Chains}{corollary.A.3}{}}
\newlabel{lemma:perron_frobenius_extension_A_to_the_m}{{A.5}{42}{}{lemma.A.5}{}}
\newlabel{remark:exponential_decay}{{A.7}{43}{}{remark.A.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Information Theory}{44}{appendix.B}\protected@file@percent }
\newlabel{section:information_theory}{{B}{44}{Information Theory}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Entropy}{44}{subsection.B.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Plot of the function $y=-x\qopname  \relax o{ln}(x)$.}}{44}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.1}Joint, Conditional, and Cross Entropy}{46}{subsubsection.B.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.2}Properties of Entropy}{47}{subsubsection.B.1.2}\protected@file@percent }
\newlabel{proposition:entropy_conditional}{{B.3}{47}{}{proposition.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Kullback-Leibler Divergence}{49}{subsection.B.2}\protected@file@percent }
\newlabel{sec:kullback_leibler_divergence}{{B.2}{49}{Kullback-Leibler Divergence}{subsection.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Mutual Information}{51}{subsection.B.3}\protected@file@percent }
\newlabel{sec:mutual_information}{{B.3}{51}{Mutual Information}{subsection.B.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3.1}Data Processing Inequality}{54}{subsubsection.B.3.1}\protected@file@percent }
\newlabel{theorem:data_processing_inequality}{{B.3}{54}{Data Processing Inequality}{theorem.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Bounding Mutual Information via Matrix Rank of the Joint Distribution}{55}{subsection.B.4}\protected@file@percent }
\newlabel{theorem:mutual_information_is_bounded_by_log_rank}{{B.4}{55}{}{theorem.B.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Convergence of Mutual Information}{56}{subsection.B.5}\protected@file@percent }
\newlabel{eq:1}{{1}{56}{Convergence of Mutual Information}{equation.B.1}{}}
\newlabel{eq:2}{{2}{56}{Convergence of Mutual Information}{equation.B.2}{}}
\newlabel{eq:3}{{3}{56}{Convergence of Mutual Information}{equation.B.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Tensor Networks}{61}{appendix.C}\protected@file@percent }
\newlabel{section:tensor_networks}{{C}{61}{Tensor Networks}{appendix.C}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A basic tensor network over $\Sigma ^3$.}}{62}{figure.16}\protected@file@percent }
\newlabel{proposition:marginalized_tensor_network}{{C.1}{63}{}{proposition.C.1}{}}
\newlabel{proposition:contracting_over_shared_index}{{C.2}{64}{}{proposition.C.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Contracting multiple tensors over one shared index is equivalent to contracting them individually with a single copy tensor.}}{65}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Reshaping a Tensor (Matricization)}{66}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1.1}An Upper Bound for the Rank of Matrices in Tensor Networks}{66}{subsubsection.C.1.1}\protected@file@percent }
\newlabel{theorem:min_cut_caps_rank}{{C.1}{67}{}{theorem.C.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A generic tensor network with a cut partitioning the physical indices into $\mathcal  {I}_A$ (blue) and $\mathcal  {I}_B$ (red). The cut crosses bonds $e_1, e_2, e_3$.}}{67}{figure.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Contracting the subgraphs results in two tensors, $U$ and $V$, connected by the cut bonds.}}{67}{figure.19}\protected@file@percent }
\gdef \@abspage@last{68}
