\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Model Framework}{4}{section.1}\protected@file@percent }
\newlabel{lemma:random_variables_do_not_change_with_future_models}{{1.1}{5}{}{lemma.1.1}{}}
\newlabel{def:induced_bulk_marginal_model}{{1.3}{5}{Induced Bulk Marginal Model}{definition.1.3}{}}
\newlabel{definition:strong_model_power_law_behavior}{{1.6}{6}{Strong Power-Law Behavior}{definition.1.6}{}}
\newlabel{definition:weak_power_law_behavior}{{1.8}{7}{Weak Power-Law Behavior}{definition.1.8}{}}
\newlabel{theorem:power_law_decay_in_well-behaved_models_with_weak_power-law_behavior}{{1.1}{7}{Every Token has Power-Law Decay in Models with the Bulk Marginal Property and Weak Power-Law Behavior}{theorem.1.1}{}}
\newlabel{proposition:strong_slbplb_implies_wlbplb}{{1.2}{8}{}{proposition.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}A Model with Power-Law Behavior}{9}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Model}{9}{subsection.2.1}\protected@file@percent }
\newlabel{definition:the_model}{{2.1}{9}{The Model}{definition.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}A Formula for Mutual Information}{9}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{proposition:marginal_distributions_of_a_normal_distribution}{{2.1}{10}{Marginal Distributions of a Normal Distribution}{proposition.2.1}{}}
\newlabel{eq:gauss_mutual_information}{{1}{11}{A Formula for Mutual Information}{equation.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Initializing Parameters}{11}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{sec:initializing_parameters}{{2.1.2}{11}{Initializing Parameters}{subsubsection.2.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Ensure Positive Definiteness}{12}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{eq:diagonal_dominant}{{2}{12}{Ensure Positive Definiteness}{equation.2}{}}
\newlabel{eq:diagonal_dominant_sufficient}{{3}{12}{Ensure Positive Definiteness}{equation.3}{}}
\newlabel{lemma:bounding_rho}{{2.1}{13}{}{lemma.2.1}{}}
\newlabel{eq:e_to_the_minus_ex_bound}{{4}{13}{Ensure Positive Definiteness}{equation.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Strong Power-Law Behavior}{14}{subsubsection.2.1.4}\protected@file@percent }
\newlabel{eq:mutual_information_in_terms_of_f}{{5}{17}{Strong Power-Law Behavior}{equation.5}{}}
\newlabel{lemma:convex_function_bound}{{2.3}{18}{}{lemma.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Summary}{20}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Mutual Information in Markov Chains}{21}{section.3}\protected@file@percent }
\newlabel{sec:mutual_information_in_markov_chains}{{3}{21}{Mutual Information in Markov Chains}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Exponential Decay in Irreducible Aperiodic Markov Chains}{21}{subsection.3.1}\protected@file@percent }
\newlabel{theorem:no_power-law_in_irreducible_aperdioc_markov_chains}{{3.1}{21}{No Power-Law in irreducible aperiodic Markov Chains}{theorem.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simple irreducible aperiodic Markov chain. Note that if $X_{t_0} = C$, then we know that $X_{t_0+1} = A$.}}{22}{figure.1}\protected@file@percent }
\newlabel{fig:2_markov_chain}{{1}{22}{A simple irreducible aperiodic Markov chain. Note that if $X_{t_0} = C$, then we know that $X_{t_0+1} = A$}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}The Defective Case}{26}{subsubsection.3.1.1}\protected@file@percent }
\newlabel{section:the_defectvie_case}{{3.1.1}{26}{The Defective Case}{subsubsection.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}No Markov Chain with Power-Law Behavior}{27}{subsection.3.2}\protected@file@percent }
\newlabel{theorem:no_markov_chain_with_power-law_behavior}{{3.2}{28}{No Markov Chain with Power-Law Behavior}{theorem.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}No Power-Law in Hidden Markov Models}{30}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bayesian network of Markov chains. All arrows go from previous tokens to future tokens.}}{30}{figure.2}\protected@file@percent }
\newlabel{fig:bayesian_network_markov_chain}{{2}{30}{Bayesian network of Markov chains. All arrows go from previous tokens to future tokens}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Bayesian network of a hidden Markov model.}}{31}{figure.3}\protected@file@percent }
\newlabel{fig:bayesian_network_hidden_markov}{{3}{31}{Bayesian network of a hidden Markov model}{figure.3}{}}
\newlabel{lemma:raising_marov_matrix_to_its_period}{{4.1}{31}{}{lemma.4.1}{}}
\newlabel{lemma:irreducible_aperiodic_markov_chain_stays_irreducible_aperiodic}{{4.2}{32}{}{lemma.4.2}{}}
\newlabel{lemma:exponential_convergence_with_open_states}{{4.4}{33}{}{lemma.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adjusted Bayesian network of a hidden Markov model.}}{35}{figure.4}\protected@file@percent }
\newlabel{fig:adjusted_bayesian_network_hidden_markov}{{4}{35}{Adjusted Bayesian network of a hidden Markov model}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Conclusions for Model Selection}{37}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Binary Tree Tensor Networks}{39}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Binary tree model space for sequences of length $n = 2^k$.}}{39}{figure.5}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network}{{5}{39}{Binary tree model space for sequences of length $n = 2^k$}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Bulk Marginal Property}{39}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Bulk marginal property enforces the equivalence of these models.}}{40}{figure.6}\protected@file@percent }
\newlabel{fig:bmp_model_equiv}{{6}{40}{Bulk marginal property enforces the equivalence of these models}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Contracting this sub-network with all-ones vectors yields vector $\bm  {v}$.}}{40}{figure.7}\protected@file@percent }
\newlabel{fig:sufficient_condition_bmp}{{7}{40}{Contracting this sub-network with all-ones vectors yields vector $\bm {v}$}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Binary Tree Tensor Networks are Universal Approximators}{41}{subsection.5.2}\protected@file@percent }
\newlabel{theorem:bttn_are_universal_approximators}{{5.1}{41}{}{theorem.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Model structure of binary tree tensor networks for $n = 2^k = 4$.}}{42}{figure.8}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network_n_equals_four}{{8}{42}{Model structure of binary tree tensor networks for $n = 2^k = 4$}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Restricting Parameters}{42}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}PCBTTN Aren't Universal Approximators}{43}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Cut in a binary tree tensor network.}}{44}{figure.9}\protected@file@percent }
\newlabel{pic:cut_bttn}{{9}{44}{Cut in a binary tree tensor network}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Power-Law Behavior in PCBTTN}{44}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Model structure of binary tree tensor networks for $n = 2^k = 4$.}}{45}{figure.10}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network_n_equals_four_2}{{10}{45}{Model structure of binary tree tensor networks for $n = 2^k = 4$}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Markov Chains}{47}{appendix.A}\protected@file@percent }
\newlabel{section:markov_chains}{{A}{47}{Markov Chains}{appendix.A}{}}
\newlabel{ex:markov_chain}{{A.1}{47}{Markov Transition Matrix}{example.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Properties}{48}{subsection.A.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Graph representation of the Markov chain defined in example~\ref {ex:markov_chain}.}}{49}{figure.11}\protected@file@percent }
\newlabel{fig:markov_chain}{{11}{49}{Graph representation of the Markov chain defined in example~\ref {ex:markov_chain}}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.1}Irreducibility}{49}{subsubsection.A.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Graph of a reducible Markov chain. Note that once the chain transitions from state $1$ to state $3$, it will stay at state $3$ indefinitely.}}{49}{figure.12}\protected@file@percent }
\newlabel{fig:markov_chain_reducible}{{12}{49}{Graph of a reducible Markov chain. Note that once the chain transitions from state $1$ to state $3$, it will stay at state $3$ indefinitely}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.2}Aperiodicity}{51}{subsubsection.A.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Graph of a periodic Markov chain. Note that once once the starting position is determined, then we also know the state after $t$ steps.}}{51}{figure.13}\protected@file@percent }
\newlabel{fig:markov_chain_periodic}{{13}{51}{Graph of a periodic Markov chain. Note that once once the starting position is determined, then we also know the state after $t$ steps}{figure.13}{}}
\newlabel{lemma:aux}{{A.3}{52}{}{lemma.A.3}{}}
\newlabel{remark:converse_lemma_aux}{{A.5}{52}{}{remark.A.5}{}}
\newlabel{lemma:return_states_with_period}{{A.4}{52}{}{lemma.A.4}{}}
\newlabel{proposition:period_is_a_class_property}{{A.1}{52}{Period is a Class Property}{proposition.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Irreducible Aperiodic Markov Chains}{53}{subsection.A.2}\protected@file@percent }
\newlabel{theorem:positive_transition_matrix}{{A.3}{53}{Positive n-Step Transition Matrix for Irreducible Aperiodic Markov Chains}{theorem.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Graph of a reducible Markov chain with two closed communication classes. Note that we might end up stuck at either state $2$ or state $3$.}}{54}{figure.14}\protected@file@percent }
\newlabel{fig:markov_chain_two_closed_classes}{{14}{54}{Graph of a reducible Markov chain with two closed communication classes. Note that we might end up stuck at either state $2$ or state $3$}{figure.14}{}}
\newlabel{corollary:converse_positive_transition_matrix}{{A.2}{54}{}{corollary.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Perron-Frobenius Theorem}{54}{subsection.A.3}\protected@file@percent }
\newlabel{theorem:perron_frobenius}{{A.4}{54}{Perron-Frobenius}{theorem.A.4}{}}
\newlabel{corollary:perron_frobenius_extension}{{A.3}{56}{Extension to More General Markov Chains}{corollary.A.3}{}}
\newlabel{lemma:perron_frobenius_extension_A_to_the_m}{{A.5}{57}{}{lemma.A.5}{}}
\newlabel{remark:exponential_decay}{{A.7}{57}{}{remark.A.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Information Theory}{58}{appendix.B}\protected@file@percent }
\newlabel{section:information_theory}{{B}{58}{Information Theory}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Entropy}{58}{subsection.B.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Plot of the function $y=-x\ln (x)$.}}{58}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.1}Joint, Conditional, and Cross Entropy}{60}{subsubsection.B.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.2}Properties of Entropy}{61}{subsubsection.B.1.2}\protected@file@percent }
\newlabel{proposition:entropy_conditional}{{B.3}{61}{}{proposition.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Kullback-Leibler Divergence}{63}{subsection.B.2}\protected@file@percent }
\newlabel{sec:kullback_leibler_divergence}{{B.2}{63}{Kullback-Leibler Divergence}{subsection.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Mutual Information}{65}{subsection.B.3}\protected@file@percent }
\newlabel{sec:mutual_information}{{B.3}{65}{Mutual Information}{subsection.B.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3.1}Data Processing Inequality}{68}{subsubsection.B.3.1}\protected@file@percent }
\newlabel{theorem:data_processing_inequality}{{B.3}{68}{Data Processing Inequality}{theorem.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Bounding Mutual Information via Matrix Rank of the Joint Distribution}{69}{subsection.B.4}\protected@file@percent }
\newlabel{theorem:mutual_information_is_bounded_by_log_rank}{{B.4}{69}{}{theorem.B.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Convergence of Mutual Information}{70}{subsection.B.5}\protected@file@percent }
\newlabel{theorem:element-wise_exponential_convergence_implies_exponential_convergence}{{B.5}{70}{Element-Wise Exponential Convergence Implies Exponential Convergence}{theorem.B.5}{}}
\newlabel{lemma:xlogx_exponential_convergence}{{B.4}{72}{}{lemma.B.4}{}}
\newlabel{corollary:exponential_convergence_of_conditional_probability_matrix_implies_exponential_convergence_of_mutual_information}{{B.9}{73}{}{corollary.B.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Tensor Networks}{74}{appendix.C}\protected@file@percent }
\newlabel{section:tensor_networks}{{C}{74}{Tensor Networks}{appendix.C}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A basic tensor network over $\Sigma ^3$.}}{75}{figure.16}\protected@file@percent }
\newlabel{proposition:marginalized_tensor_network}{{C.1}{76}{}{proposition.C.1}{}}
\newlabel{corollary:normalized_tensor_network}{{C.1}{76}{}{corollary.C.1}{}}
\newlabel{proposition:contracting_over_shared_index}{{C.2}{77}{}{proposition.C.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Contracting multiple tensors over one shared index is equivalent to contracting them individually with a single copy tensor.}}{78}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Reshaping a Tensor (Matricization)}{79}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1.1}An Upper Bound for the Rank of Matrices in Tensor Networks}{79}{subsubsection.C.1.1}\protected@file@percent }
\newlabel{theorem:min_cut_caps_rank}{{C.1}{80}{}{theorem.C.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A generic tensor network with a cut partitioning the physical indices into $\mathcal  {I}_A$ (blue) and $\mathcal  {I}_B$ (red). The cut crosses bonds $e_1, e_2, e_3$.}}{80}{figure.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Contracting the subgraphs results in two tensors, $U$ and $V$, connected by the cut bonds.}}{80}{figure.19}\protected@file@percent }
\gdef \@abspage@last{81}
