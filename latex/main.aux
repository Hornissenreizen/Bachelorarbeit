\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Model Framework}{3}{section.1}\protected@file@percent }
\newlabel{lemma:random_variables_do_not_change_with_future_models}{{1.1}{4}{}{lemma.1.1}{}}
\newlabel{def:induced_bulk_marginal_model}{{1.3}{4}{Induced Bulk Marginal Model}{definition.1.3}{}}
\newlabel{definition:strong_model_power_law_behavior}{{1.6}{5}{Strong Power-Law Behavior}{definition.1.6}{}}
\newlabel{definition:weak_power_law_behavior}{{1.8}{6}{Weak Power-Law Behavior}{definition.1.8}{}}
\newlabel{theorem:power_law_decay_in_well-behaved_models_with_weak_power-law_behavior}{{1.1}{6}{Every Token has Power-Law Decay in Models with the Bulk Marginal Property and Weak Power-Law Behavior}{theorem.1.1}{}}
\newlabel{proposition:strong_slbplb_implies_wlbplb}{{1.2}{7}{}{proposition.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mutual Information in Markov Chains}{8}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Exponential Decay in Irreducible Aperiodic Markov Chains}{8}{subsection.2.1}\protected@file@percent }
\newlabel{theorem:no_power-law_in_irreducible_aperdioc_markov_chains}{{2.1}{8}{No Power-Law in irreducible aperiodic Markov Chains}{theorem.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A simple irreducible aperiodic Markov chain. Note that if $X_{t_0} = C$, then we know that $X_{t_0+1} = A$.}}{9}{figure.1}\protected@file@percent }
\newlabel{fig:2_markov_chain}{{1}{9}{A simple irreducible aperiodic Markov chain. Note that if $X_{t_0} = C$, then we know that $X_{t_0+1} = A$}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}The Defective Case}{13}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{section:the_defectvie_case}{{2.1.1}{13}{The Defective Case}{subsubsection.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}No Markov Chain with Power-Law Behavior}{14}{subsection.2.2}\protected@file@percent }
\newlabel{theorem:no_markov_chain_with_power-law_behavior}{{2.2}{15}{No Markov Chain with Power-Law Behavior}{theorem.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}No Power-Law in Hidden Markov Models}{17}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bayesian network of Markov chains. All arrows go from previous tokens to future tokens.}}{17}{figure.2}\protected@file@percent }
\newlabel{fig:bayesian_network_markov_chain}{{2}{17}{Bayesian network of Markov chains. All arrows go from previous tokens to future tokens}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Bayesian network of a hidden Markov model.}}{18}{figure.3}\protected@file@percent }
\newlabel{fig:bayesian_network_hidden_markov}{{3}{18}{Bayesian network of a hidden Markov model}{figure.3}{}}
\newlabel{lemma:raising_marov_matrix_to_its_period}{{3.1}{18}{}{lemma.3.1}{}}
\newlabel{lemma:irreducible_aperiodic_markov_chain_stays_irreducible_aperiodic}{{3.2}{19}{}{lemma.3.2}{}}
\newlabel{lemma:exponential_convergence_with_open_states}{{3.4}{20}{}{lemma.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Adjusted Bayesian network of a hidden Markov model.}}{22}{figure.4}\protected@file@percent }
\newlabel{fig:adjusted_bayesian_network_hidden_markov}{{4}{22}{Adjusted Bayesian network of a hidden Markov model}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Conclusions for Model Selection}{24}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Binary Tree Tensor Networks}{26}{section.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Binary tree model space for sequences of length $n = 2^k$.}}{26}{figure.5}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network}{{5}{26}{Binary tree model space for sequences of length $n = 2^k$}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Bulk Marginal Property}{26}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Bulk marginal property enforces the equivalence of these models.}}{27}{figure.6}\protected@file@percent }
\newlabel{fig:bmp_model_equiv}{{6}{27}{Bulk marginal property enforces the equivalence of these models}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Contracting this sub-network with all-ones vectors yields vector $\bm  {v}$.}}{27}{figure.7}\protected@file@percent }
\newlabel{fig:sufficient_condition_bmp}{{7}{27}{Contracting this sub-network with all-ones vectors yields vector $\bm {v}$}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Binary Tree Tensor Networks are Universal Approximators}{28}{subsection.4.2}\protected@file@percent }
\newlabel{theorem:bttn_are_universal_approximators}{{4.1}{28}{}{theorem.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Model structure of binary tree tensor networks for $n = 2^k = 4$.}}{29}{figure.8}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network_n_equals_four}{{8}{29}{Model structure of binary tree tensor networks for $n = 2^k = 4$}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Restricting Parameters}{29}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}PCBTTN Aren't Universal Approximators}{30}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Cut in a binary tree tensor network.}}{31}{figure.9}\protected@file@percent }
\newlabel{pic:cut_bttn}{{9}{31}{Cut in a binary tree tensor network}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Power-Law Behavior in PCBTTN}{31}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Model structure of binary tree tensor networks for $n = 2^k = 4$.}}{32}{figure.10}\protected@file@percent }
\newlabel{fig:binary_tree_tensor_network_n_equals_four_2}{{10}{32}{Model structure of binary tree tensor networks for $n = 2^k = 4$}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Markov Chains}{34}{appendix.A}\protected@file@percent }
\newlabel{section:markov_chains}{{A}{34}{Markov Chains}{appendix.A}{}}
\newlabel{ex:markov_chain}{{A.1}{34}{Markov Transition Matrix}{example.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Properties}{35}{subsection.A.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Graph representation of the Markov chain defined in example~\ref {ex:markov_chain}.}}{36}{figure.11}\protected@file@percent }
\newlabel{fig:markov_chain}{{11}{36}{Graph representation of the Markov chain defined in example~\ref {ex:markov_chain}}{figure.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.1}Irreducibility}{36}{subsubsection.A.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Graph of a reducible Markov chain. Note that once the chain transitions from state $1$ to state $3$, it will stay at state $3$ indefinitely.}}{36}{figure.12}\protected@file@percent }
\newlabel{fig:markov_chain_reducible}{{12}{36}{Graph of a reducible Markov chain. Note that once the chain transitions from state $1$ to state $3$, it will stay at state $3$ indefinitely}{figure.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.2}Aperiodicity}{38}{subsubsection.A.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Graph of a periodic Markov chain. Note that once once the starting position is determined, then we also know the state after $t$ steps.}}{38}{figure.13}\protected@file@percent }
\newlabel{fig:markov_chain_periodic}{{13}{38}{Graph of a periodic Markov chain. Note that once once the starting position is determined, then we also know the state after $t$ steps}{figure.13}{}}
\newlabel{lemma:aux}{{A.3}{39}{}{lemma.A.3}{}}
\newlabel{remark:converse_lemma_aux}{{A.5}{39}{}{remark.A.5}{}}
\newlabel{lemma:return_states_with_period}{{A.4}{39}{}{lemma.A.4}{}}
\newlabel{proposition:period_is_a_class_property}{{A.1}{39}{Period is a Class Property}{proposition.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Irreducible Aperiodic Markov Chains}{40}{subsection.A.2}\protected@file@percent }
\newlabel{theorem:positive_transition_matrix}{{A.3}{40}{Positive n-Step Transition Matrix for Irreducible Aperiodic Markov Chains}{theorem.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Graph of a reducible Markov chain with two closed communication classes. Note that we might end up stuck at either state $2$ or state $3$.}}{41}{figure.14}\protected@file@percent }
\newlabel{fig:markov_chain_two_closed_classes}{{14}{41}{Graph of a reducible Markov chain with two closed communication classes. Note that we might end up stuck at either state $2$ or state $3$}{figure.14}{}}
\newlabel{corollary:converse_positive_transition_matrix}{{A.2}{41}{}{corollary.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Perron-Frobenius Theorem}{41}{subsection.A.3}\protected@file@percent }
\newlabel{theorem:perron_frobenius}{{A.4}{41}{Perron-Frobenius}{theorem.A.4}{}}
\newlabel{corollary:perron_frobenius_extension}{{A.3}{43}{Extension to More General Markov Chains}{corollary.A.3}{}}
\newlabel{lemma:perron_frobenius_extension_A_to_the_m}{{A.5}{44}{}{lemma.A.5}{}}
\newlabel{remark:exponential_decay}{{A.7}{44}{}{remark.A.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Information Theory}{46}{appendix.B}\protected@file@percent }
\newlabel{section:information_theory}{{B}{46}{Information Theory}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Entropy}{46}{subsection.B.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Plot of the function $y=-x\ln (x)$.}}{46}{figure.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.1}Joint, Conditional, and Cross Entropy}{48}{subsubsection.B.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.1.2}Properties of Entropy}{49}{subsubsection.B.1.2}\protected@file@percent }
\newlabel{proposition:entropy_conditional}{{B.3}{49}{}{proposition.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Kullback-Leibler Divergence}{51}{subsection.B.2}\protected@file@percent }
\newlabel{sec:kullback_leibler_divergence}{{B.2}{51}{Kullback-Leibler Divergence}{subsection.B.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Mutual Information}{53}{subsection.B.3}\protected@file@percent }
\newlabel{sec:mutual_information}{{B.3}{53}{Mutual Information}{subsection.B.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {B.3.1}Data Processing Inequality}{56}{subsubsection.B.3.1}\protected@file@percent }
\newlabel{theorem:data_processing_inequality}{{B.3}{56}{Data Processing Inequality}{theorem.B.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Bounding Mutual Information via Matrix Rank of the Joint Distribution}{57}{subsection.B.4}\protected@file@percent }
\newlabel{theorem:mutual_information_is_bounded_by_log_rank}{{B.4}{57}{}{theorem.B.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Convergence of Mutual Information}{58}{subsection.B.5}\protected@file@percent }
\newlabel{theorem:element-wise_exponential_convergence_implies_exponential_convergence}{{B.5}{58}{Element-Wise Exponential Convergence Implies Exponential Convergence}{theorem.B.5}{}}
\newlabel{lemma:xlogx_exponential_convergence}{{B.4}{60}{}{lemma.B.4}{}}
\newlabel{corollary:exponential_convergence_of_conditional_probability_matrix_implies_exponential_convergence_of_mutual_information}{{B.9}{61}{}{corollary.B.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Tensor Networks}{62}{appendix.C}\protected@file@percent }
\newlabel{section:tensor_networks}{{C}{62}{Tensor Networks}{appendix.C}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A basic tensor network over $\Sigma ^3$.}}{63}{figure.16}\protected@file@percent }
\newlabel{proposition:marginalized_tensor_network}{{C.1}{64}{}{proposition.C.1}{}}
\newlabel{corollary:normalized_tensor_network}{{C.1}{64}{}{corollary.C.1}{}}
\newlabel{proposition:contracting_over_shared_index}{{C.2}{65}{}{proposition.C.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Contracting multiple tensors over one shared index is equivalent to contracting them individually with a single copy tensor.}}{66}{figure.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Reshaping a Tensor (Matricization)}{67}{subsection.C.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1.1}An Upper Bound for the Rank of Matrices in Tensor Networks}{67}{subsubsection.C.1.1}\protected@file@percent }
\newlabel{theorem:min_cut_caps_rank}{{C.1}{68}{}{theorem.C.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces A generic tensor network with a cut partitioning the physical indices into $\mathcal  {I}_A$ (blue) and $\mathcal  {I}_B$ (red). The cut crosses bonds $e_1, e_2, e_3$.}}{68}{figure.18}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Contracting the subgraphs results in two tensors, $U$ and $V$, connected by the cut bonds.}}{68}{figure.19}\protected@file@percent }
\gdef \@abspage@last{69}
