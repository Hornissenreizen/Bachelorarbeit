\documentclass[../../main.tex]{subfiles}

\begin{document}
\chapter{Technical Details}
\label{section:technical_details}
In this appendix we provide additional details to our arguments.

\section{Quadrant Probabilities for the Bivariate Normal Distribution}
\label{section:integrating_quadrants}
In order to derive a formula for the quadrant probabilities of $\mathcal{N}\left(\bm{0}, \begin{pmatrix*}
    1 & \rho \\
    \rho & 1 \\
\end{pmatrix*}\right)$, we first have to prove an auxiliary lemma:

\begin{lemma}
Let $X$ and $Y$ have a bivariate normal distribution where $X$ and $Y$ are standard normal variables, $X, Y \sim \mathcal{N}(0,1)$, with correlation $\rho$. The variable $Z = \frac{Y-\rho X}{\sqrt{1-\rho^2}}$ is a standard normal variable, and $X$ and $Z$ are independent.
\end{lemma}

\begin{proof}
First, we show that $Z$ is a standard normal variable. Since $Z$ is a linear combination of the jointly normal variables $X$ and $Y$, $Z$ is also a normal variable. We compute its mean and variance.

The mean of $Z$ is:
$$ E[Z] = E\left[\frac{Y-\rho X}{\sqrt{1-\rho^2}}\right] = \frac{E[Y]-\rho E[X]}{\sqrt{1-\rho^2}} = \frac{0 - \rho \cdot 0}{\sqrt{1-\rho^2}} = 0 \quad .$$
The variance of $Z$ is:
\begin{align*}
    \text{Var}(Z) &= \text{Var}\left(\frac{Y-\rho X}{\sqrt{1-\rho^2}}\right) = \frac{1}{1-\rho^2}\text{Var}(Y-\rho X) \\
    &= \frac{1}{1-\rho^2}\left(\text{Var}(Y) + \rho^2\text{Var}(X) - 2\rho\text{Cov}(X,Y)\right) \quad .
\end{align*}
Since $X$ and $Y$ are standard normal variables, $\text{Var}(X) = 1$, $\text{Var}(Y) = 1$, and their covariance $\text{Cov}(X,Y)$ is equal to their correlation $\rho$. Hence:
$$ \text{Var}(Z) = \frac{1}{1-\rho^2}(1 + \rho^2(1) - 2\rho(\rho)) = \frac{1-\rho^2}{1-\rho^2} = 1 \quad . $$
Thus, $Z$ is a standard normal variable, $Z \sim \mathcal{N}(0,1)$.

To show that $X$ and $Z$ are independent, we compute their covariance. Since they are jointly normal, zero covariance implies independence.
\begin{align*}
    \text{Cov}(X,Z) &= \text{Cov}\left(X, \frac{Y-\rho X}{\sqrt{1-\rho^2}}\right) = \frac{1}{\sqrt{1-\rho^2}}\text{Cov}(X, Y-\rho X) \\
    &= \frac{1}{\sqrt{1-\rho^2}}\left(\text{Cov}(X,Y) - \rho\text{Cov}(X,X)\right) \\
    &= \frac{1}{\sqrt{1-\rho^2}}\left(\rho - \rho\text{Var}(X)\right) = \frac{1}{\sqrt{1-\rho^2}}(\rho - \rho \cdot 1) = 0 \quad .
\end{align*}
Since $\text{Cov}(X,Z)=0$ and they are jointly normal, $X$ and $Z$ are independent.
\end{proof}

Now we can prove our proposition of interest:

\begin{proposition}
For bivariate standard normal variables $X$ and $Y$ with correlation $\rho$,
$$ P(X>0, Y>0) = \frac{1}{4} + \frac{1}{2\pi}\arcsin(\rho) \quad . $$
\end{proposition}

\begin{proof}
Define the random variable $Z$ like in the previous lemma. Then, the event $\{X>0, Y>0\}$ is the same as the event $\{X>0, Z > \frac{-\rho}{\sqrt{1-\rho^2}}X\}$, where $X$ and $Z$ are independent standard normal variables as shown above. Writing $a := \frac{-\rho}{\sqrt{1-\rho^2}}$ for brevity, the desired probability is expressible as a double integral involving the joint density of $(X,Z)$:
\begin{align*}
    P(X>0, Y>0) &= P(X>0, Z>aX) \\
    &= \int_{x=0}^{\infty} \int_{z=ax}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-x^2/2} \frac{1}{\sqrt{2\pi}}e^{-z^2/2} \,dz\,dx \quad .
\end{align*}
Switching to polar coordinates ($x=r\cos\theta, z=r\sin\theta$), the integral becomes:
$$ \int_{\theta=\arctan(a)}^{\pi/2} \int_{r=0}^{\infty} \frac{1}{2\pi} e^{-r^2/2} r \,dr\,d\theta = \int_{\theta=\arctan(a)}^{\pi/2} \frac{1}{2\pi} \left[-e^{-r^2/2}\right]_0^\infty \,d\theta \quad . $$
This equals:
$$ \int_{\theta=\arctan(a)}^{\pi/2} \frac{1}{2\pi} \,d\theta = \frac{1}{2\pi}\left(\frac{\pi}{2} - \arctan(a)\right) = \frac{1}{4} - \frac{1}{2\pi}\arctan\left(\frac{-\rho}{\sqrt{1-\rho^2}}\right) \quad  .$$
Using the fact that the arctan function is odd, i.e. $\arctan(-u) = -\arctan(u)$, we get:
$$ \frac{1}{4} + \frac{1}{2\pi}\arctan\left(\frac{\rho}{\sqrt{1-\rho^2}}\right) \quad . $$
To finish, we use the identity $\arcsin(\rho) = \arctan\left(\frac{\rho}{\sqrt{1-\rho^2}}\right)$. To see this, let $\phi = \arcsin(\rho)$ for $\phi \in [-\pi/2, \pi/2]$. Then $\sin(\phi) = \rho$ and $\cos(\phi) = \sqrt{1-\rho^2}$. Thus, $\tan(\phi) = \frac{\sin(\phi)}{\cos(\phi)} = \frac{\rho}{\sqrt{1-\rho^2}}$, which implies $\phi = \arctan\left(\frac{\rho}{\sqrt{1-\rho^2}}\right)$.
Substituting this into our expression gives the final result:
$$ P(X>0, Y>0) = \frac{1}{4} + \frac{1}{2\pi}\arcsin(\rho) \quad . $$
\end{proof}

\section{Prerequisites for Theorem~\ref{theorem:no_hidden_markov_model_with_power-law}}
\label{section:prerequisites_for_theorem}
\begin{lemma}
    \label{lemma:exponential_convergence_with_open_states}
    Let
    \[
        \bm{M} \coloneqq
        \begin{bmatrix}
            \bm{A} & \bm{B} \\
            \bm{0} & \bm{C}
        \end{bmatrix}
    \]
    be a matrix consisting of submatrices \( \bm{A} \in \mathbb{R}^{k \times k} \), \( \bm{B} \in \mathbb{R}^{k \times \ell} \), and \( \bm{C} \in \mathbb{R}^{\ell \times \ell} \). Let $\bm{A}$ be an irreducible aperiodic Markov transition matrix, and let $\bm{C}^n \xrightarrow{n \to \infty} \bm{0}$ with exponential decay.
    Then, $\bm{M}^n$ decays exponentially in $n$ towards a matrix $\bm{M}'$.
\end{lemma}

\begin{proof}
The proof proceeds in three steps. First, we establish a closed-form expression for $\bm{M}^n$. Second, we determine the limit matrix $\bm{M}'$. Third, we prove that the convergence to this limit is exponential.

\paragraph{1. The Form of \(\bm{M}^n\)}
Since $\bm{M}$ is a block upper triangular matrix, its powers take a specific form. By induction, we can show that:
\[
    \bm{M}^n = 
    \begin{bmatrix}
        \bm{A}^n & \bm{X}_n \\
        \bm{0} & \bm{C}^n
    \end{bmatrix}
    \quad \text{where} \quad
    \bm{X}_n = \sum_{j=0}^{n-1} \bm{A}^{n-1-j} \bm{B} \bm{C}^j
    \quad .
\]

\paragraph{2. The Limit Matrix \(\bm{M}'\)}
We analyze the limit of each block of $\bm{M}^n$ as $n \to \infty$.

\begin{itemize}
    \item \textbf{Block \(\bm{A}^n\):} Since $\bm{A}$ is an irreducible aperiodic Markov transition matrix, the Perron-Frobenius theorem for stochastic matrices guarantees that $\bm{A}^n$ converges to a rank-one matrix $\bm{A}' = \bm{\pi}\bm{1}^T$, where $\bm{\pi}$ is the unique stationary distribution. The convergence is exponential, so there exist constants $K_A > 0$ and $0 \le \lambda < 1$ such that $\|\bm{A}^n - \bm{A}'\| \le K_A \lambda^n$.

    \item \textbf{Block \(\bm{C}^n\):} By hypothesis, $\bm{C}^n \to \bm{0}$ with exponential decay. This is equivalent to its spectral radius being less than one, $\rho(\bm{C}) < 1$. Thus, there exist constants $K_C > 0$ and $0 \le \gamma < 1$ such that $\|\bm{C}^n\| \le K_C \gamma^n$.

    \item \textbf{Block \(\bm{X}_n\):} Let $\bm{E}_k = \bm{A}^k - \bm{A}'$. We have $\|\bm{E}_k\| \le K_A\lambda^k$. We can rewrite $\bm{X}_n$ as:
    \[
        \bm{X}_n = \sum_{j=0}^{n-1} (\bm{A}' + \bm{E}_{n-1-j}) \bm{B} \bm{C}^j = \bm{A}'\bm{B} \left( \sum_{j=0}^{n-1} \bm{C}^j \right) + \sum_{j=0}^{n-1} \bm{E}_{n-1-j} \bm{B} \bm{C}^j \quad .
    \]
    As $n \to \infty$, the first term converges to $\bm{A}'\bm{B}(\bm{I}-\bm{C})^{-1}$, since the series $\sum_{j=0}^\infty \bm{C}^j$ converges to $(\bm{I}-\bm{C})^{-1}$. The second term converges to $\bm{0}$ because its norm is bounded by a vanishing convolution sum. Thus, the limit of $\bm{X}_n$ is $\bm{X}' = \bm{A}'\bm{B}(\bm{I}-\bm{C})^{-1}$.
\end{itemize}
Combining these limits, the limit matrix is $\bm{M}' = \begin{bsmallmatrix} \bm{A}' & \bm{X}' \\ \bm{0} & \bm{0} \end{bsmallmatrix}$.

\paragraph{3. Exponential Rate of Convergence}
We analyze the norm of the difference matrix $\bm{M}^n - \bm{M}'$:
\[
    \bm{M}^n - \bm{M}' = 
    \begin{bmatrix}
        \bm{A}^n - \bm{A}' & \bm{X}_n - \bm{X}' \\
        \bm{0} & \bm{C}^n
    \end{bmatrix}
    \quad .
\]
The blocks $\|\bm{A}^n - \bm{A}'\|$ and $\|\bm{C}^n\|$ decay exponentially by definition. We examine the convergence of the off-diagonal block:
\[
    \bm{X}_n - \bm{X}' = -\bm{A}'\bm{B}\left(\sum_{j=n}^{\infty}\bm{C}^j\right) + \sum_{j=0}^{n-1}\bm{E}_{n-1-j}\bm{B}\bm{C}^j \quad .
\]
The norm of each part is bounded by an exponentially decaying function:
\begin{itemize}
    \item $\left\|-\bm{A}'\bm{B}\left(\sum_{j=n}^{\infty}\bm{C}^j\right)\right\| \le \|\bm{A}'\|\|\bm{B}\| \sum_{j=n}^{\infty}\|\bm{C}^j\| \le \|\bm{A}'\|\|\bm{B}\| K_C \frac{\gamma^n}{1-\gamma}$. This decays with rate $\gamma$.
    \item $\left\|\sum_{j=0}^{n-1}\bm{E}_{n-1-j}\bm{B}\bm{C}^j\right\| \le \sum_{j=0}^{n-1} K_A\lambda^{n-1-j} \|\bm{B}\| K_C\gamma^j = K_A \|\bm{B}\| K_C \sum_{j=0}^{n-1} \lambda^{n-1-j} \gamma^j$. This convolution sum is bounded by $K'n\mu^n$ where $\mu = \max(\lambda, \gamma)$, which decays exponentially.
\end{itemize}
Since $\|\bm{X}_n - \bm{X}'\|$ is bounded by a sum of exponentially decaying terms, it also decays exponentially. As all blocks of $\bm{M}^n - \bm{M}'$ converge to zero exponentially, the norm $\|\bm{M}^n - \bm{M}'\|$ does as well. This completes the proof.
\end{proof}

\clearpage
\subsection{Convergence of Mutual Information}
\begin{theorem}[Element-Wise Exponential Convergence Implies Exponential Convergence]
    \label{theorem:element-wise_exponential_convergence_implies_exponential_convergence}
Let $f: \mathcal{D} \to \mathbb{R}^m$ be a function defined on a convex domain $\mathcal{D} \subseteq \mathbb{R}^n$ that is a Cartesian product of real intervals, i.e., $\mathcal{D} = \mathcal{D}_1 \times \mathcal{D}_2 \times \dots \times \mathcal{D}_n$ where each $\mathcal{D}_i \subseteq \mathbb{R}$ is an interval. Let $\{\bm{x}_k\}_{k=1}^\infty \subset \mathcal{D}$ be a sequence converging exponentially fast to $\bm{x}_0 \in \mathcal{D}$.

Let $\bm{e}_j$ denote the $j$-th standard basis vector in $\mathbb{R}^n$. Suppose that for each input coordinate $j \in \{1, 2, \dots, n\}$ there exist functions $K_j(C', \rho)$, $C_j(C', \rho)$, $P_j(C', \rho)$ s.t. for every sequence $\{\bm{u}_k\}_{k=1}^\infty \subset \mathcal{D}$ converging to $\bm{u}_0$ where the difference $\bm{u}_\ell-\bm{u}_{\ell'}$ is parallel to $\bm{e}_j$ (i.e., they only differ in the $j$-th coordinate) that satisfies $|\bm{u}_0 - \bm{u}_k| \leq C' \rho^k$ for all $k$ and some $\rho \in [0, 1), \ C' > 0$, we have for all $k \geq K_j(C', \rho)$:
\[
    \|f(\bm{u}_0) - f(\bm{u}_k)\| \le C_j(C', \rho) \rho^k k^{P_j(C', \rho)} \quad .
\]
Then, there exist constants $C > 0$ and $\sigma \in [0, 1)$ such that for all sufficiently large $k$:
\[
    \|f(\bm{x}_0) - f(\bm{x}_k)\| \le C \sigma^k \quad .
\]
\end{theorem}

\begin{proof}
Let the sequence $\{\bm{x}_k\}_{k=1}^\infty \subset \mathcal{D}$ converge exponentially to $\bm{x}_0 \in \mathcal{D}$. By definition, there exist constants $C_x > 0$ and $\rho \in [0, 1)$ such that for all $k$,
\[
    \|\bm{x}_k - \bm{x}_0\| \le C_x \rho^k \quad .
\]
Let $\bm{x}_k = (x_{k,1}, \dots, x_{k,n})^T$ and $\bm{x}_0 = (x_{0,1}, \dots, x_{0,n})^T$. An immediate consequence is that each coordinate also converges exponentially, i.e., for each $j \in \{1, \dots, n\}$:
\[
    |x_{k,j} - x_{0,j}| \le \|\bm{x}_k - \bm{x}_0\|_{\infty} \le \|\bm{x}_k - \bm{x}_0\| \le C_x \rho^k \quad ,
\]
where we use the equivalence of norms in $\mathbb{R}^n$.

To bound $\|f(\bm{x}_0) - f(\bm{x}_k)\|$, we define a sequence of $n+1$ intermediate points that form a path from $\bm{x}_k$ to $\bm{x}_0$ by changing one coordinate at a time. For each $k$, let:
\begin{align*}
    \bm{z}_{k,0} &:= \bm{x}_k = (x_{k,1}, x_{k,2}, \dots, x_{k,n}) \\
    \bm{z}_{k,1} &:= (x_{0,1}, x_{k,2}, \dots, x_{k,n}) \\
    & \vdots \\
    \bm{z}_{k,j} &:= (x_{0,1}, \dots, x_{0,j}, x_{k,j+1}, \dots, x_{k,n}) \\
    & \vdots \\
    \bm{z}_{k,n} &:= (x_{0,1}, \dots, x_{0,n}) = \bm{x}_0 \quad .
\end{align*}
Since $\mathcal{D}$ is a cartesian product intervals and both $\bm{x}_k$ and $\bm{x}_0$ are in $\mathcal{D}$, all intermediate points $\bm{z}_{k,j}$ are also contained in $\mathcal{D}$. We can express the total difference $f(\bm{x}_0) - f(\bm{x}_k)$ as a telescoping sum:
\[
    f(\bm{x}_0) - f(\bm{x}_k) = f(\bm{z}_{k,n}) - f(\bm{z}_{k,0}) = \sum_{j=1}^{n} \left( f(\bm{z}_{k,j}) - f(\bm{z}_{k,j-1}) \right) \quad .
\]
By the triangle inequality, we have:
\[
    \|f(\bm{x}_0) - f(\bm{x}_k)\| \le \sum_{j=1}^{n} \|f(\bm{z}_{k,j}) - f(\bm{z}_{k,j-1})\| \quad .
\]
Now, we analyze each term $\|f(\bm{z}_{k,j}) - f(\bm{z}_{k,j-1})\|$ for a fixed $j \in \{1, \dots, n\}$. The points $\bm{z}_{k,j}$ and $\bm{z}_{k,j-1}$ differ only in their $j$-th coordinate.

Let us define a sequence $\{\bm{u}_m\}_{m=1}^\infty$ and a limit point $\bm{u}_0$ that fit the condition in the theorem's hypothesis. For the given $j$ and $k$, let
\begin{align*}
    \bm{u}_m &:= (x_{0,1}, \dots, x_{0,j-1}, x_{m,j}, x_{k,j+1}, \dots, x_{k,n}) \\
    \bm{u}_0 &:= (x_{0,1}, \dots, x_{0,j-1}, x_{0,j}, x_{k,j+1}, \dots, x_{k,n}) \quad .
\end{align*}
Note that $\bm{u}_0 = \bm{z}_{k,j}$ and by setting $m=k$, we get $\bm{u}_k = \bm{z}_{k,j-1}$.
The sequence $\{\bm{u}_m\}$ lies on a line parallel to the $j$-th coordinate axis. As $m \to \infty$, $\bm{u}_m \to \bm{u}_0$ because $x_{m,j} \to x_{0,j}$. The convergence is exponential:
\[
    \|\bm{u}_m - \bm{u}_0\| = |x_{m,j} - x_{0,j}| \le C_x \rho^m \quad .
\]
The hypothesis states that for any such sequence, there exist constants $K_j(C_x, \rho)$, $C_j(C_x, \rho)$, $P_j(C_x, \rho)$ which are independent of the specific line, such that for all $k \geq K_j(C_x, \rho)$ we have $\|f(\bm{u}_0) - f(\bm{u}_m)\| \le C_j(C_x, \rho) \rho^m m^{P_j(C_x, \rho)}$. Applying this for $m=k$:
\[
    \|f(\bm{z}_{k,j}) - f(\bm{z}_{k,j-1})\| = \|f(\bm{u}_0) - f(\bm{u}_k)\| \leq C_j(C_x, \rho) \rho^k k^{P_j(C_x, \rho)} \quad .
\]
This inequality holds for each $j=1, \dots, n$. Substituting these bounds back into the sum:
\[
    \|f(\bm{x}_0) - f(\bm{x}_k)\| \le \sum_{j=1}^{n} C_j(C_x, \rho) \rho^k k^{P_j(C_x, \rho)} \quad .
\]
Let $K \coloneqq \max_{j \in \{1, \dots, n\}} \{K_j(C_x, \rho)\}$, $C \coloneqq \sum_{j=1}^{n} C_j(C_x, \rho)$ and \\
$P \coloneqq \max_{j \in \{1, \dots, n\}} \{P_j(C_x, \rho)\}$. Hence, for all $k \geq K$:
\[
    \|f(\bm{x}_0) - f(\bm{x}_k)\| \le \sum_{j=1}^{n} C_j(C_x, \rho) \rho^k k^P = \left( \sum_{j=1}^{n} C_j(C_x, \rho) \right) \rho^k k^P = C \rho^k k^P \quad .
\]
This shows that $\{f(\bm{x}_k)\}$ converges exponentially to $f(\bm{x}_0)$ with a rate of $\sigma$ s.t. $\rho < \sigma < 1$ (like $\sigma \coloneqq \sqrt{\rho}$). This completes the proof.
\end{proof}

\begin{lemma}
\label{lemma:xlogx_exponential_convergence}
Let the function $f: [0, \infty) \to \mathbb{R}$ be defined as $f(x) = x \log x$ for $x \in (0, 1)$, and $f(x) = 0$ everywhere else. If a sequence $\{x_k\}_{k=1}^\infty \subset [0,1]$ converging to a limit $x_\infty \in [0,1]$ satisfies $|x_k - x_\infty| \leq C \rho^k$ for some $C \in \mathbb{R}_{>0}, \ \rho \in [0, 1)$, then the sequence $\{f(x_k)\}$ converges to $f(x_\infty)$ with $|f(x_k) - f(x_\infty)| \leq C \rho^k \ |\log C + k \log \rho|$ for $k \geq \log_\rho \left( \dfrac{e^{-1}}{C} \right) \eqqcolon K$.
\end{lemma}
\vspace{-2.5em}
\begin{proof}
We consider two cases for the limit $x_\infty$:

\textbf{Case 1: $x_\infty = 0$} \\
In this case, $|x_k - 0| = x_k \le C \rho^k$. We want to bound the difference $|f(x_k) - f(0)| = |f(x_k)|$.
Note that $|f(x)|$ is monotonically increasing on $[0, e^{-1}]$. For $k \geq K$ we have $x_k \leq C \rho^k \leq e^{-1}$. Thus, for all $k \geq K$:
\[
    |f(x_k)| \leq |f(C \rho^k)| = |C \rho^k \log (C \rho^k)| = C \rho^k \ |\log C + k \log \rho| \quad .
\]

\textbf{Case 2: $x_\infty > 0$} \\
Similarly, for $k \geq K$ we have $|x_k - x_\infty| \leq C \rho^k \leq e^{-1}$. Based on the function graph, it follows that for $k \geq K$ we have:
\[
    |f(x_k) - f(x_\infty)| \leq |f(C \rho^k) - f(0)| = |f(C \rho^k)| = C \rho^k \ |\log C + k \log \rho| \quad .
\]
\end{proof}

\begin{theorem}[Element-Wise Exponential Convergence Property of Mutual Information]
Let the function $f: [0, \infty) \to \mathbb{R}$ be defined as $f(x) = x \log x$ for $x \in (0, 1)$, and $f(x) = 0$ everywhere else. Define the function $I: [0, 1]^{m \times n} \to \mathbb{R}$ for a matrix $\bm{M} \in [0, 1]^{m \times n}$ as:
\begin{align*}
    I(\bm{M}) &= \sum_{i=1}^m \sum_{j=1}^n f(M_{ij}) - \sum_{i=1}^m f\left(\sum_{j=1}^n M_{ij}\right) - \sum_{j=1}^n f\left(\sum_{i=1}^m M_{ij}\right) \quad .
\end{align*}
This function exhibits element-wise exponential convergence. That is, for any single component $(i_0, j_0)$, if a sequence of matrices $\{\bm{U}_k\}_{k=1}^\infty \subset [0, 1]^{m \times n}$ converges to a limit $\bm{U}_\infty$, varies only in the $(i_0, j_0)$-th component and satisfies $\|\bm{U}_k - \bm{U}_\infty\| \leq C \rho^k$ for some $C > 0$, $\rho \in [0, 1)$ and all $k$, then the sequence of values $\{I(\bm{U}_k)\}$ converges to $I(\bm{U}_\infty)$ with $|I(\bm{U}_k) - I(\bm{U}_\infty)| \leq C'(C, \rho) \rho^k k^{P(C, \rho)}$ for all $k \geq K(C, \rho)$.
\end{theorem}
% \vspace{-2.5em}
\begin{proof}
The function $I(\bm{M})$ is a sum of terms involving $f$ applied to the matrix entries and their row and column sums. Let $m_i(\bm{M}) = \sum_j M_{ij}$ and $m'_j(\bm{M}) = \sum_i M_{ij}$. We have:
\[
    I(\bm{M}) = \sum_{i,j} f(M_{ij}) - \sum_i f(m_i(\bm{M})) - \sum_j f(m'_j(\bm{M})) \quad .
\]
We are given a sequence $\{\bm{U}_k\}$ that varies only in the $(i_0, j_0)$-th component, $u_k = \bm{U}_k(i_0, j_0)$. All other components are constant. The exponential convergence of $\{\bm{U}_k\}$ means $|u_k - u_\infty| \le C \rho^k$.

\clearpage
The difference $I(\bm{U}_k) - I(\bm{U}_\infty)$ consists only of terms whose arguments change with $k$. These are:
\begin{enumerate}
    \item The entry term: $f(u_k)$.
    \item The row-sum term: $f(m_{i_0}(\bm{U}_k))$, where $m_{i_0}(\bm{U}_k) = u_k + \text{const}$.
    \item The column-sum term: $f(m'_{j_0}(\bm{U}_k))$, where $m'_{j_0}(\bm{U}_k) = u_k + \text{const}$.
\end{enumerate}
By the triangle inequality, the total error is bounded by the sum of the absolute errors of these three terms:
\begin{align*}
|I(\bm{U}_k) - I(\bm{U}_\infty)| \le \ & |f(u_k) - f(u_\infty)| \\
& + |f(m_{i_0}(\bm{U}_k)) - f(m_{i_0}(\bm{U}_\infty))| \\
& + |f(m'_{j_0}(\bm{U}_k)) - f(m'_{j_0}(\bm{U}_\infty))| \quad .
\end{align*}
The arguments to the function $f$ in each of these three terms converge exponentially to their limits with rate $\rho$ and constant $C$, since $|m_{i_0}(\bm{U}_k) - m_{i_0}(\bm{U}_\infty)| = |u_k - u_\infty|$ and $|m'_{j_0}(\bm{U}_k) - m'_{j_0}(\bm{U}_\infty)| = |u_k - u_\infty|$.

Hence, by lemma~\ref{lemma:xlogx_exponential_convergence}, we have:
\begin{align*}
|I(\bm{U}_k) - I(\bm{U}_\infty)| &\leq 3 C \rho^k \ |\log C + k \log \rho| \\
&\leq 3 C \rho^k k \ (|\log C| + |\log \rho|) \\
&= C' \rho^k k \quad ,
\end{align*}
with $C' \coloneqq 3 C (|\log C| + |\log \rho|)$ and for all $k \geq K(C, \rho)$. Note that $K$, $C'$ and $P$ only depend on $C$ and $\rho$.
\end{proof}

\begin{corollary}
    Using theorem~\ref{theorem:element-wise_exponential_convergence_implies_exponential_convergence}, we see that if a sequence $\{\bm{P}_k\}$ of joint probability distributions converges exponentially fast, then $\{I(\bm{P}_k)\}$ converges exponentially fast as well.
\end{corollary}

\begin{corollary}
    \label{corollary:exponential_convergence_of_conditional_probability_matrix_implies_exponential_convergence_of_mutual_information}
    A joint probability matrix $\bm{P}_{X,Y}$ can be calculated from the conditional probability matrix $\bm{P}_{Y \mid X}$ and the diagonal matrix $\bm{P}_{X}$ with the probabilities for $X$ on its diagonal using $\bm{P}_{X,Y} = \bm{P}_{Y \mid X} \bm{P}_{X}$. Hence, if $\bm{P}_{Y \mid X}$ converges exponentially fast while $\bm{P}_X$ stays constant, the mutual information $I(X;Y)$ will converge exponentially fast as well.
\end{corollary}
\end{document}
